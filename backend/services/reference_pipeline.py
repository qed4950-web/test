import pandas as pd
from sqlalchemy.orm import Session
from fastapi import UploadFile, HTTPException
import io
import json
from datetime import datetime
from backend import models, schemas
from .rule_vectorizer import rule_vectorizer

def process_file_upload(file: UploadFile, org_id: str, db: Session):
    contents = file.file.read()
    
    try:
        if file.filename.endswith('.csv'):
            df = pd.read_csv(io.BytesIO(contents))
        elif file.filename.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(io.BytesIO(contents))
        else:
            raise HTTPException(status_code=400, detail="Invalid file format")
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Error reading file: {str(e)}")

    created_refs = []
    
    # Expected columns: name, menu_category, price, etc.
    # We will map whatever we can and put the rest in metadata_json
    
    for _, row in df.iterrows():
        # 1. Validation: Mandatory fields
        if pd.isna(row.get('name')) or pd.isna(row.get('menu_category')):
             # Log warning or skip? Let's skip and maybe log count
             continue
             
        name = row.get('name')
        
        # 2. Duplicate Check
        existing = db.query(models.Reference).filter(
            models.Reference.org_id == org_id,
            models.Reference.name == name
        ).first()
        
        if existing:
            # Skip duplicates
            continue

        raw_data = row.to_dict()
        # Clean NaNs
        raw_data = {k: v for k, v in raw_data.items() if pd.notna(v)}
        
        # Default fields
        ref_db = models.Reference(
            id=models.generate_uuid(),
            org_id=org_id,
            name=raw_data.get('name'),
            menu_category=raw_data.get('menu_category', 'Uncategorized'),
            reference_type=schemas.ReferenceType.BRAND, # Default to BRAND for uploads
            source_kind=schemas.SourceKind.MARKET,
            status=schemas.ReferenceStatus.ACTIVE,
            process_status=schemas.ReferenceProcessStatus.QUEUED,
            metadata_json=raw_data # Store everything
        )
        
        # Parse 'keywords' if present
        if 'keywords' in raw_data and isinstance(raw_data['keywords'], str):
             # "Spicy, Sweet" -> ["Spicy", "Sweet"]
             ref_db.metadata_json['keywords'] = [k.strip() for k in raw_data['keywords'].split(',')]
        
        db.add(ref_db)
        created_refs.append(ref_db)
    
    db.commit()
    return created_refs

def run_metric_estimation(reference_ids: list[str], db_session_factory):
    # This runs in background
    db = db_session_factory()
    try:
        refs = db.query(models.Reference).filter(models.Reference.id.in_(reference_ids)).all()
        for ref in refs:
            ref.process_status = schemas.ReferenceProcessStatus.RUNNING
            db.commit()
            
            # Simulate LLM Processing or actually call it
            # For now, we mock basic extraction from name/metadata
            
            try:
                # Mock estimation logic
                estimated_taste = estimate_taste_metrics(ref.name, ref.metadata_json)
                
                # Rule-based Vectorization
                keywords = []
                if ref.metadata_json and 'keywords' in ref.metadata_json:
                    keywords = ref.metadata_json['keywords']
                # Fallback to name if no keywords
                if not keywords:
                    keywords = [ref.name]
                
                vector = rule_vectorizer.vectorize_from_keywords(keywords)

                # Create a fingerprint if not exists
                fp = models.ReferenceFingerprint(
                    id=models.generate_uuid(),
                    reference_id=ref.id,
                    version=1,
                    vector=vector,
                    metrics_json=estimated_taste,
                    notes="Auto-generated by Rule Vectorizer"
                )
                db.add(fp)
                
                ref.process_status = schemas.ReferenceProcessStatus.COMPLETED
            except Exception as e:
                ref.process_status = schemas.ReferenceProcessStatus.FAILED
                # Update metadata with error?
                if ref.metadata_json:
                    ref.metadata_json['error'] = str(e)
                else:
                    ref.metadata_json = {'error': str(e)}
            
        db.commit()
    finally:
        db.close()

def estimate_taste_metrics(name: str, metadata: dict):
    # TODO: Replace with actual LLM call
    # Simple keyword heuristic for now
    name_lower = name.lower()
    
    metrics = {
        "taste": {"salt": 0.5, "sweet": 0.5, "sour": 0.1, "bitter": 0.1, "umami": 0.5},
        "texture": {"fat": 0.5, "crisp": 0.1, "juiciness": 0.5},
        "aroma": {"fire": 0.0, "garlic": 0.0, "spice": 0.0},
        "behavior": {"addictiveness": 0.5}
    }
    
    if "spicy" in name_lower or "hot" in name_lower or "불" in name_lower:
        metrics["taste"]["salt"] += 0.2
        metrics["aroma"]["spice"] = 0.8
        metrics["aroma"]["fire"] = 0.6
        metrics["behavior"]["addictiveness"] = 0.8

    if "sweet" in name_lower or "honey" in name_lower or "꿀" in name_lower:
        metrics["taste"]["sweet"] = 0.9

    if "garlic" in name_lower or "마늘" in name_lower:
        metrics["aroma"]["garlic"] = 0.9
        
    return metrics
